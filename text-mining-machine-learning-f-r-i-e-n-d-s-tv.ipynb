{"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMi0S0ASKgRQ4/o/uf0mHip"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3017794,"sourceType":"datasetVersion","datasetId":1848343}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"TFM FRIENDS","metadata":{"id":"BN6xdZV4xJ1h"}},{"cell_type":"markdown","source":"Autora: Mar Burgueño Martín\nMáster Big Data & Data Science\nUniversidad Complutense de Madrid","metadata":{}},{"cell_type":"markdown","source":"**ANÁLISIS DESCRIPTIVO DEL CONJUNTO DE DATOS (EDA)**\n","metadata":{"id":"9ocL7OPFxUX_"}},{"cell_type":"code","source":"#Importación de librerías y herramientas\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nfrom gensim import corpora\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\nimport seaborn as sns\n\n\n# Descargar recursos de NLTK\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('vader_lexicon')\n","metadata":{"executionInfo":{"elapsed":4279,"status":"ok","timestamp":1709196443420,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"},"user_tz":-60},"id":"ma8Vzc2b-YDS","outputId":"25b21381-87d2-4461-f93a-1acc98afffbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import files\n#Carga el archivo desde tu PC\nuploaded = files.upload()\n\n#Obtiene el nombre del archivo cargado\nfile_name = next(iter(uploaded))\n\n#Lee el archivo CSV en un DataFrame de Pandas\ndf = pd.read_csv(\"Friends.csv\")\n\n\n#Mostrar las primeras filas del DataFrame\nprint(\"Primeras filas del DataFrame:\")\nprint(df.head())\n\n#Obtener información sobre el DataFrame\nprint(\"\\nInformación del DataFrame:\")\nprint(df.info())\n\n#Verificar si hay valores nulos\nprint(\"\\nValores nulos en el DataFrame:\")\nprint(df.isnull().sum())\n\n#Verificar duplicados\nprint(\"\\nNúmero de duplicados en el DataFrame:\", df.duplicated().sum())\n\n#Obtener tamaño del dataset\nprint(\"\\nTamaño del dataset:\")\nprint(df.shape)\n","metadata":{"id":"Q6ppBep4xJF3","outputId":"cff43a41-f7db-425d-dfda-12cfacab76c7","executionInfo":{"status":"ok","timestamp":1709196563540,"user_tz":-60,"elapsed":114896,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*   Vemos que el conjunto de datos contiene 69.974 filas y cinco columnas: Text, Speaker, Episode, Season y Show. Ya desde el primer momento podríamos descartar la columna Show ya que hace referencia a Friends y no aporta más información.\n*   Hay valores nulos en la columna Speaker, con 6.284 valores faltantes. Esto cabe suponer que hay diálogos sin asociar a ningún personaje específico.\n* Hay 858 filas duplicadas en el conjunto de datos.\n","metadata":{"id":"Zq1h-_fizgP5"}},{"cell_type":"markdown","source":"Antes de seguir con el análisis vamos a tratar los valores nulos y los duplicados. Los valores nulos los imputaremos por 'Unknown' y los duplicados los eliminaremos.","metadata":{"id":"_AusCVH61Die"}},{"cell_type":"code","source":"#Imputar valores nulos en la columna \"Speaker\"\ndf['Speaker'].fillna('Unknown', inplace=True)\n\n#Verificar que ya no hay valores nulos en la columna \"Speaker\"\nprint(\"Valores nulos en la columna 'Speaker' después de imputar:\", df['Speaker'].isnull().sum())\n\n#Eliminar filas duplicadas\ndf.drop_duplicates(inplace=True)\n\n#Verificar que ya no hay duplicados\nprint(\"Número de duplicados en el DataFrame después de eliminarlos:\", df.duplicated().sum())\n\n#Obtener el nuevo tamaño del dataset después de eliminar duplicados\nprint(\"\\nNuevo tamaño del dataset después de eliminar duplicados:\", df.shape)\n\n","metadata":{"id":"A1IWqC9DxM5R","executionInfo":{"status":"ok","timestamp":1709196589517,"user_tz":-60,"elapsed":6,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"1935f5c1-e770-4e51-ee13-f77012eb300f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez tratados los valores nulos y duplicados seguimos con el análisis. Visualizamos la distribución de los personajes (Speakers) en los diálogos de Friends.","metadata":{"id":"h12Vz_Av2HXk"}},{"cell_type":"markdown","source":"Antes vamos a agrupar los personajes principales, que son los que nos interesan a la hora de llevar a cabo este proyecto. En una serie como Friends hay  muchísimos personajes secundarios, cameos de una sola vez, etc que no aportan información relevante.","metadata":{"id":"tmhZGYoM3ocV"}},{"cell_type":"code","source":"df['Speaker']","metadata":{"id":"QEwbRzCF4Arx","executionInfo":{"status":"ok","timestamp":1709196592045,"user_tz":-60,"elapsed":257,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"bfe67111-28a6-4818-d6a7-20e5ef7cced2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convertimos todas las cadenas de texto en minúsculas antes de filtrar el df para que aparezcan todas las variantes de nombres. Visualizamos la **Distribución de Personajes Principales en los Diálogos de Friends:**","metadata":{"id":"dw0zXczl4R8o"}},{"cell_type":"code","source":"#Convertir todos los nombres de personajes a minúsculas\ndf['Speaker'] = df['Speaker'].str.lower()\n\n#Lista de protagonistas principales en minúsculas\npersonajes_principales = ['joey', 'monica', 'rachel', 'phoebe', 'ross', 'chandler']\n\n#Filtrar el DataFrame para incluir solo los diálogos de los protagonistas\ndf_personajes_principales = df[df['Speaker'].isin(personajes_principales)]\n\n#Calcular la frecuencia de cada hablante\nspeaker_counts_protagonistas = df_personajes_principales['Speaker'].value_counts()\n\n#Graficar la distribución de hablantes de los protagonistas\nplt.figure(figsize=(10, 6))\nspeaker_counts_protagonistas.plot(kind='bar', color='lightcoral')\nplt.title('Distribución de Personajes Principales en los Diálogos de Friends')\nplt.xlabel('Hablante')\nplt.ylabel('Frecuencia')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"FmaE1_vV4c_Z","executionInfo":{"status":"ok","timestamp":1709196595271,"user_tz":-60,"elapsed":529,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"9ff94730-8273-45ef-b85d-fea2684f57e7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este gráfico se puede apreciar el protagonismo de ciertos personajes frente a otros. Ross es el que más cantidad de texto tiene a lo largo de toda la serie, muy seguido de Rachel y Joey, mientras que Monica y Phoebe son las que más atrás se quedan en comparación con sus compañeros.","metadata":{"id":"-g0EmDt2QVit"}},{"cell_type":"markdown","source":"**Análisis de las palabras más frecuentes en los diálogos de Friends**","metadata":{"id":"wgl1KehV9n4L"}},{"cell_type":"code","source":"#Obtener una lista de todas las palabras en los diálogos\nall_words = ' '.join(df['Text']).lower().split()\n\n#Eliminar las palabras vacías (stop words)\nstop_words = set(stopwords.words('english'))\nfiltered_words = [word for word in all_words if word not in stop_words]\n\n#Crear una cadena de texto a partir de las palabras filtradas\ntext = ' '.join(filtered_words)\n\n#Crear la nube de palabras\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\n#Visualizar la nube de palabras\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Nube de Palabras: Palabras más Frecuentes en los Diálogos de Friends')\nplt.axis('off')\nplt.show()\n","metadata":{"id":"PSNAvOY_96Yl","executionInfo":{"status":"ok","timestamp":1709196605142,"user_tz":-60,"elapsed":5497,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"fe194732-bd01-4a6e-c19e-3c290aae6cd6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La nube de palabras muestra las más frecuentes en los diálogos de la serie. Las palabras más grandes son las más frecuentes, mientras que las pequeñas son menos frecuentes.\n\n\n*   Temas principales: aparecen todos los nombres de los protagonistas. El predominante sería Ross y le seguirían Monica, Joey, Phoebe Chandler y Rachel.\n*   Palabras comunes como yeah, oh, okay, know, well, guy, sorry, oh god... son muy habituales. Son palabras comunes en el lenguaje coloquial y sirven para expresar emociones, confirmar entendimiento o facilitar una conversación fluida. Se puede atribuir a la serie un tono coloquial, uso frecuente del lenguaje sencillo y cercano y la exploración de temas comunes y cotidianos relacionado con la vida de jóvenes adultos en Nueva York.  \n\n","metadata":{"id":"2KpjIsuA-L2V"}},{"cell_type":"markdown","source":"Longitud de los diálogos por temporada","metadata":{"id":"ZlJG_hSgOh6g"}},{"cell_type":"code","source":"#Calcular la longitud de los diálogos por temporada\ndf_personajes_principales['Dialogue_Length'] = df_personajes_principales['Text'].apply(lambda x: len(x.split()))\n\n#Calcular la longitud promedio de los diálogos por temporada\navg_dialogue_length_season = df_personajes_principales.groupby('Season')['Dialogue_Length'].mean()\n\n#Visualizar la longitud promedio de los diálogos por temporada\nplt.figure(figsize=(10, 6))\navg_dialogue_length_season.plot(kind='bar', color='lightseagreen')\nplt.title('Longitud Promedio de los Diálogos por Temporada')\nplt.xlabel('Temporada')\nplt.ylabel('Longitud Promedio de los Diálogos')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"F6UAAI6AOlSv","executionInfo":{"status":"ok","timestamp":1709196608963,"user_tz":-60,"elapsed":721,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"d4ae7c0c-393c-45f6-c225-4e1026f57fe3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La longitud promedio de los diálogos por temporada se mantiene bastante estable a lo largo de las temporadas. Destacan ligeramente por encima del resto en longitud las temporadas 6 y 9. Ambas temporadas tienen momentos importantes para la serie como es la pedida de matrimonio entre Monica y Chandler (temporada 6), mientras que la novena temporada es la penúltima de la serie y ocurren muchas tramas entre todos los personajes.","metadata":{"id":"YMSdn9s1O5oi"}},{"cell_type":"markdown","source":"**Análisis de la frecuencia de los personajes en cada temporada**","metadata":{"id":"WaodGHL3_vIU"}},{"cell_type":"code","source":"#Lista de personajes principales\npersonajes_principales = ['joey', 'monica', 'rachel', 'phoebe', 'ross', 'chandler']\n\n#Filtrar el DataFrame para incluir solo los personajes principales\ndf_personajes_principales = df[df['Speaker'].str.lower().isin(personajes_principales)]\n\n#Contar la frecuencia de cada personaje en cada temporada\ncharacter_counts = df_personajes_principales.groupby(['Season', 'Speaker']).size().unstack(fill_value=0)\n\n#Visualizar los resultados en un gráfico de barras\nplt.figure(figsize=(12, 6))\ncharacter_counts.plot(kind='bar', stacked=True)\nplt.title('Frecuencia de Personajes Principales por Temporada')\nplt.xlabel('Temporada')\nplt.ylabel('Frecuencia')\nplt.xticks(rotation=45, ha='right')\nplt.legend(title='Personaje')\nplt.tight_layout()\nplt.show()","metadata":{"id":"XLFMOdpe_zGM","executionInfo":{"status":"ok","timestamp":1709196612940,"user_tz":-60,"elapsed":749,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"a3d6e438-6f3f-4966-a1c8-f4b432e84853"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos que todos los personajes principales tienen una presencia constante a lo largo de las temporadas, por lo que se demuestra el papel relevante y recurrente de todos en la serie. Sin embargo, hay variaciones en la frecuencia en diferentes temporadas.\n\n*   Ross y Rachel destacan y su aparición en la serie es ligeramente más alta que la de otros personajes principales en varias temporadas. Esto puede reflejar su relación y la trama asociada en la serie.\n*   Aun así, se obseva un equilibrio entre los personajes y esto sugiere que Friends se centra en la interacción entre las historias de todos los protagonistas de manera equitativa.\n\n","metadata":{"id":"fTpdyFwiA8fR"}},{"cell_type":"markdown","source":"Relaciones entre los personajes","metadata":{"id":"wMas9NhiUSzj"}},{"cell_type":"markdown","source":"Vamos a crear un gráfico que visualice las relaciones entre los protagonistas para tener una visión más completa sobre la dinámica entre los personajes.","metadata":{"id":"NQNpWB1GUVzL"}},{"cell_type":"code","source":"#Crear una lista de protagonistas\nprotagonists_list = df_personajes_principales['Speaker'].unique()\n\n#Crear un DataFrame para almacenar las interacciones de cada protagonista con los demás\ninteractions_with_others = pd.DataFrame(index=protagonists_list, columns=protagonists_list)\n\n#Calcular las interacciones de cada protagonista con los demás\nfor protagonist in protagonists_list:\n    # Filtrar las interacciones del protagonista actual con los demás\n    interactions_with_others[protagonist] = df_personajes_principales[df_personajes_principales['Speaker'] != protagonist]['Speaker'].value_counts()\n\n#Visualizar las interacciones de cada protagonista con los demás como un gráfico de barras\nplt.figure(figsize=(12, 8))\ninteractions_with_others.plot(kind='bar', stacked=True, cmap='Paired')\nplt.title('Frecuencia de Interacciones entre Protagonistas en Toda la Serie')\nplt.xlabel('Protagonista')\nplt.ylabel('Frecuencia de Interacciones')\nplt.xticks(rotation=45)\nplt.legend(title='Protagonista')\nplt.tight_layout()\nplt.show()\n\n","metadata":{"id":"X9VjuEu2Y8pf","executionInfo":{"status":"ok","timestamp":1709196617045,"user_tz":-60,"elapsed":796,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"17550d38-a0f3-4405-c3a2-5ba63aea2b22"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este gráfico se pueden apreciar los personajes que interactúan más con otros. Por ejemplo, Ross y Rachel.","metadata":{"id":"bVgRf6vxZxnC"}},{"cell_type":"markdown","source":"**PREPROCESAMIENTO DE LOS DATOS**","metadata":{"id":"-EivST84H77Z"}},{"cell_type":"markdown","source":"Vamos a proceder a la tokenización, eliminación de caracteres especiales y de stopwords, lematización y conversión a minúsculas.","metadata":{"id":"gphoYwm7S6fI"}},{"cell_type":"code","source":"#Inicializar lematizador\nlemmatizer = WordNetLemmatizer()\n\n#Función para eliminar números\ndef remove_numbers(text):\n    return ''.join([i for i in text if not i.isdigit()])\n\n#Tokenización\ndf['Text'] = df['Text'].apply(word_tokenize)\n\n#Conversión a minúsculas\ndf['Text'] = df['Text'].apply(lambda x: [word.lower() for word in x])\n\n#Eliminación de caracteres no deseados y números\npunctuation = set(string.punctuation)\ndf['Text'] = df['Text'].apply(lambda x: [remove_numbers(word) for word in x if word not in punctuation])\n\n#Eliminación de stopwords\nstop_words = set(stopwords.words('english'))\ndf['Text'] = df['Text'].apply(lambda x: [word for word in x if word not in stop_words])\n\n#Lematización\ndf['Text'] = df['Text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n\n#Unir las palabras nuevamente en una sola cadena\ndf['Text'] = df['Text'].apply(lambda x: ' '.join(x))\n","metadata":{"id":"zEzLNDBzH_AG","executionInfo":{"status":"ok","timestamp":1709196640050,"user_tz":-60,"elapsed":20344,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ANÁLISIS DE SENTIMIENTO**","metadata":{"id":"WNKD0cYiT9Vi"}},{"cell_type":"markdown","source":"Vamos a contrastar diferentes diccionarios de sentimientos y evaluaremos cómo se diferencian sus puntajes. Utilizaremos Vader y TextBlob para asignar puntajes de sentimiento a algunas palabras de ejemplo.","metadata":{"id":"LbvEKuFxUBR2"}},{"cell_type":"code","source":"#Inicializar Vader\nsid = SentimentIntensityAnalyzer()\n\n#Palabras de ejemplo\nexample_words = [\"happy\", \"sad\", \"excited\", \"angry\"]\n\n#Puntajes de sentimiento en Vader\nvader_scores = {word: sid.polarity_scores(word)['compound'] for word in example_words}\n\nprint(\"Palabra\\t\\tVader\")\nprint(\"--------------------\")\nfor word in example_words:\n    print(f\"{word}\\t\\t{vader_scores[word]}\")\n","metadata":{"id":"KRYoxH8zWoA3","executionInfo":{"status":"ok","timestamp":1709196642909,"user_tz":-60,"elapsed":245,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"ac092601-22c1-469e-9858-64937ee8961b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Palabras de ejemplo\nexample_words = [\"happy\", \"sad\", \"excited\", \"angry\"]\n\n#Puntajes de sentimiento en TextBlob\ntextblob_scores = {word: TextBlob(word).sentiment.polarity for word in example_words}\n\nprint(\"\\nPalabra\\t\\tTextBlob\")\nprint(\"--------------------\")\nfor word in example_words:\n    print(f\"{word}\\t\\t{textblob_scores[word]}\")\n","metadata":{"id":"8qDUlNm2W2gu","executionInfo":{"status":"ok","timestamp":1709196646261,"user_tz":-60,"elapsed":397,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"83267a15-322d-4a90-a6dd-b15ced77615e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que hay diferencias en los puntajes de sentimiento asignados por Vader y TextBlob en las mismas palabras.\n\n\n*   'Happy': ambos diccionarios consideran que es una palabra positiva, pero TextBlob le asigna una puntuación mucho más alta.\n*   Para el resto de palabras ambos diccionarios asignan puntajes bastante similares.\n\n","metadata":{"id":"XGC6wU2XX0oW"}},{"cell_type":"markdown","source":"Comenzamos con el análisis de sentimiento de los textos de Friends. Seguiremos utilizando ambos diccionarios para contrastar resultados","metadata":{"id":"zEGK41JQak5k"}},{"cell_type":"code","source":"#Inicializar Vader\nsid = SentimentIntensityAnalyzer()\n\n#Aplicar Vader a los diálogos de Friends\ndf['Sentiment'] = df['Text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n\n#Crear una función para asignar la categoría de sentimiento\ndef categorize_sentiment(score):\n    if score > 0:\n        return 'positivo'\n    elif score < 0:\n        return 'negativo'\n    else:\n        return 'neutro'\n#Aplicar la función a los puntajes de sentimiento compuesto\ndf['Sentiment'] = df['Sentiment'].apply(categorize_sentiment)\n\nfrom textblob import TextBlob\n# Definir una función para asignar la categoría de sentimiento en TextBlob\ndef categorize_sentiment_textblob(score):\n    if score > 0:\n        return 'positivo'\n    elif score < 0:\n        return 'negativo'\n    else:\n        return 'neutro'\n#Aplicar TextBlob a los diálogos de Friends\ndf['Sentiment_TextBlob'] = df['Text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n# Aplicar la función de categorización a los puntajes de sentimiento en TextBlob\ndf['Sentiment_TextBlob'] = df['Sentiment_TextBlob'].apply(categorize_sentiment_textblob)\n","metadata":{"id":"Y0Dgsju-arsT","executionInfo":{"status":"ok","timestamp":1709196669082,"user_tz":-60,"elapsed":20518,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizamos los resultados obtenidos con ambos diccionarios:","metadata":{"id":"lghrUCLbbAc6"}},{"cell_type":"code","source":"#Configurar el tamaño de la figura\nplt.figure(figsize=(12, 6))\n\n#Histograma de los puntajes de sentimiento obtenidos con Vader\nplt.subplot(1, 2, 1)\nplt.hist(df['Sentiment'], bins=20, color='skyblue', edgecolor='black')\nplt.title('Distribución de Sentimientos (Vader)')\nplt.xlabel('Puntaje de Sentimiento')\nplt.ylabel('Frecuencia')\n\n#Histograma de los puntajes de sentimiento obtenidos con TextBlob\nplt.subplot(1, 2, 2)\nplt.hist(df['Sentiment_TextBlob'], bins=20, color='salmon', edgecolor='black')\nplt.title('Distribución de Sentimientos (TextBlob)')\nplt.xlabel('Puntaje de Sentimiento')\nplt.ylabel('Frecuencia')\n\n#Mostrar los gráficos\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"IonaePVhbDmL","executionInfo":{"status":"ok","timestamp":1709196673816,"user_tz":-60,"elapsed":772,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"8acaa525-b682-4788-aae1-3f793545bb7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ambos histogramas muestran la variedad de sentimientos en los diálgos de Friends. La tendencia principal la marca la neutralidad o la positividad en la mayoría de los casos. Vader tiende a marcar puntajes más positivos que TextBlob.","metadata":{"id":"dkK124DSbPSl"}},{"cell_type":"markdown","source":"**Análisis de sentimientos por personaje y temporada**","metadata":{"id":"CLLXwjjIdoaQ"}},{"cell_type":"code","source":"#Lista de protagonistas principales\nprotagonistas = ['joey', 'monica', 'rachel', 'phoebe', 'ross', 'chandler']\n\n#Filtrar el DataFrame para incluir solo los diálogos de los protagonistas principales\ndf_protagonistas = df[df['Speaker'].str.lower().isin(protagonistas)]\n\n#Aplicar TextBlob a los diálogos de Friends de los protagonistas principales\ndf_protagonistas['Sentiment_TextBlob'] = df_protagonistas['Text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n\n#Agrupar los puntajes de sentimiento de TextBlob por personaje y episodio y calcular los promedios\ntextblob_grouped = df_protagonistas.groupby(['Speaker', 'Season'])['Sentiment_TextBlob'].mean().reset_index()\n\n#Crear un gráfico de barras para cada protagonista principal\nfor personaje in protagonistas:\n    data_personaje = textblob_grouped[textblob_grouped['Speaker'] == personaje]\n    plt.figure(figsize=(12, 6))\n    plt.bar(data_personaje['Season'], data_personaje['Sentiment_TextBlob'])\n    plt.title(f'Puntaje Promedio de Sentimiento por Temporada ({personaje.capitalize()})')\n    plt.xlabel('Temporada')\n    plt.ylabel('Puntaje Promedio de Sentimiento')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"id":"e15nnlKli0X9","executionInfo":{"status":"ok","timestamp":1709196692518,"user_tz":-60,"elapsed":11563,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"64ac1838-43d7-486f-90f9-b461a1ec83d6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definir colores para cada personaje\ncolors = {'joey': 'black', 'phoebe': 'red', 'chandler': 'orange', 'monica': 'blue', 'ross': 'brown', 'rachel': 'green'}\n\n#Crear el gráfico de dispersión con colores personalizados\nplt.figure(figsize=(12, 8))\nfor personaje in protagonistas:\n    data_personaje = textblob_grouped[textblob_grouped['Speaker'] == personaje]\n    plt.scatter(data_personaje['Season'], data_personaje['Sentiment_TextBlob'], label=personaje, color=colors.get(personaje, 'blue'))\nplt.title('Puntaje Promedio de Sentimiento por Temporada (Gráfico de Dispersión)')\nplt.xlabel('Temporada')\nplt.ylabel('Puntaje Promedio de Sentimiento')\nplt.legend(title='Personaje')\nplt.tight_layout()\nplt.show()","metadata":{"id":"gvCJPvODD1K8","executionInfo":{"status":"ok","timestamp":1709196701465,"user_tz":-60,"elapsed":1125,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"cd5c68ae-43a8-4930-849e-868dab6f5e5a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los puntajes de sentimiento más altos se pueden ver representados en el personaje de Phoebe, en rojo en el gráfico, especialmente en las temporadas 4, 2 y 7. El mayor puntaje de sentimiento se produce en la temporada 4, en la que Phoebe se convierte en madre y gana mayor protagonismo en la trama. Por otro lado, Mónica tiene el puntaje más bajo en la temporada 1, mientras que Joey y Chandler también presentan puntajes bajos de sentimiento a lo largo de la serie.","metadata":{"id":"_l0GciZg2pdC"}},{"cell_type":"code","source":"#Agrupar los datos por personaje y calcular el promedio del sentimiento positivo\naverage_positive_sentiment = df_protagonistas[df_protagonistas['Sentiment_TextBlob'] > 0].groupby('Speaker')['Sentiment_TextBlob'].mean()\n\n#Encontrar el personaje con el mayor y el menor sentimiento positivo\nmax_positive_sentiment_character = average_positive_sentiment.idxmax()\nmin_positive_sentiment_character = average_positive_sentiment.idxmin()\n\n#Mostrar los resultados\nprint(\"Personaje con mayor sentimiento positivo:\", max_positive_sentiment_character)\nprint(\"Personaje con menor sentimiento positivo:\", min_positive_sentiment_character)\n","metadata":{"id":"S5OTVimyIzpH","executionInfo":{"status":"ok","timestamp":1709196709823,"user_tz":-60,"elapsed":292,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"616c3548-3766-421c-9f11-0aa849ff9069"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El personaje con mayor sentimiento positivo a lo largo de todas las temporadas de Friends es Phoebe, mientras que Joey es el personaje que menor sentimiento positivo presenta.","metadata":{"id":"DlkidF6R3EgA"}},{"cell_type":"markdown","source":"**ANÁLISIS DE TÓPICOS:**","metadata":{"id":"pjV2YC4QqVWD"}},{"cell_type":"markdown","source":"Vamos a llevar a cabo el modelado de LDA para identificar los temas principales en los diálogos de Friends.\n*   Creamos un diccionario de palabras a través de los textos preprocesados.\n*   Convertimos el corpus de texto en formato que pueda utilizar LDA.\n*   Entrenamos un modelo LDA con cinco tópicos.\n*   Imprimimos los tópicos y las palabras más relevantes en cada tópico.\n\n\n\n","metadata":{"id":"JoxtFaa2q9P7"}},{"cell_type":"code","source":"#Crear un diccionario de palabras a partir de los textos preprocesados de los protagonistas\ndictionary = corpora.Dictionary(df_protagonistas['Text'].apply(lambda x: x.split()))\n\n#Convertir el corpus en un formato que pueda ser utilizado por el modelo LDA\ncorpus = [dictionary.doc2bow(text.split()) for text in df_protagonistas['Text']]\n\n#Entrenar el modelo LDA\nlda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, random_state=42)\n\n#Imprimir los tópicos y las palabras más relevantes en cada tópico\nfor topic_id, topic_words in lda_model.print_topics():\n    print(f\"Topic {topic_id}: {topic_words}\")\n\n","metadata":{"id":"YLry_LFdrFWL","executionInfo":{"status":"ok","timestamp":1709196727241,"user_tz":-60,"elapsed":14033,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"bab5c12d-b452-4bdc-ac51-f917eeb76bd2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los resultados muestran los cinco tems identificados por el modelo LDA junto a sus palabras más relevantes.\n\n*   Tópico 0: Expresiones y verbos comunes. Expresiones y palabras comunes que se utilizan con frecuencia en los diálogos.\n*   Tópico 1: Expresiones de emoción y afirmación. Oh, yeah, god, thank.\n*   Tópico 2: Saludos y nombres de personajes. Hi, wait, y nombres de los personajes.\n*   Tópico 3: Expresiones y verbos comunes. Contracciones de verbos en inglés y expresiones comunes.\n*   Tópico 4: Puntuación y verbos comunes. Hey, got, yes, get.\n\n\n\n\n\n\n","metadata":{"id":"QqPlqHdtTmI3"}},{"cell_type":"code","source":"#Crear word clouds para cada tópico\nfor topic_id, topic_words in lda_model.print_topics():\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(topic_words)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(f'Word Cloud - Tópico {topic_id}')\n    plt.axis('off')\n    plt.show()\n\n#Crear gráficos de barras para la distribución de palabras clave en cada tópico\nfor topic_id, topic_words in lda_model.print_topics():\n    words = [word.split(\"*\")[1].replace('\"', '').strip() for word in topic_words.split(\" + \")]\n    prob = [float(word.split(\"*\")[0]) for word in topic_words.split(\" + \")]\n    plt.figure(figsize=(10, 5))\n    plt.bar(words, prob, color='skyblue')\n    plt.title(f'Distribución de palabras clave - Tópico {topic_id}')\n    plt.xlabel('Palabra')\n    plt.ylabel('Probabilidad')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"id":"1lZ8T16zJvEu","executionInfo":{"status":"ok","timestamp":1709196734103,"user_tz":-60,"elapsed":4022,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"362054f1-ff0c-4bb8-8c8b-a2275eed849d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MODELIZACIÓN:","metadata":{"id":"m8tFvRruVhKQ"}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport gensim\nfrom gensim import matutils\n\n#Asignación de tópicos a documentos\ntopics_distribution = [lda_model.get_document_topics(doc) for doc in corpus]\n\n# Crear una matriz de características 'X' utilizando la asignación de tópicos a documentos como características\ntopics_distribution = [lda_model.get_document_topics(doc) for doc in corpus]\nX = matutils.corpus2dense(topics_distribution, num_terms=lda_model.num_topics).T\n\n#Asegurar que X y y tengan el mismo número de muestras\ny = df['Sentiment']\ny = y[:X.shape[0]]\n\n#Dividir los datos en conjunto de entrenamiento y conjunto de prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n#Inicializar y entrenar un modelo de clasificación (por ejemplo, regresión logística)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n#Predecir sobre el conjunto de prueba\ny_pred = model.predict(X_test)\n\n#Evaluar el modelo\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)","metadata":{"id":"mxJJ-fOz-4nz","executionInfo":{"status":"ok","timestamp":1709201608236,"user_tz":-60,"elapsed":23048,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"9de67302-05ce-4751-cb76-9c899d9b3d6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probamos con otros modelos en un intento de mejorar los resultados:","metadata":{"id":"BUy_MYMVRYmT"}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n#Inicializar modelos de clasificación\nmodels = [\n    LogisticRegression(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    SVC()\n]\n\n#Nombres de los modelos para identificarlos en la salida\nmodel_names = [\n    \"Regresión Logística\",\n    \"K Vecinos Más Cercanos\",\n    \"Árbol de Decisión\",\n    \"Bosque Aleatorio\",\n    \"SVM\"\n]\n\n#Iterar sobre los modelos y entrenarlos\nfor model, name in zip(models, model_names):\n    #Entrenar el modelo\n    model.fit(X_train, y_train)\n\n    #Predecir sobre el conjunto de prueba\n    y_pred = model.predict(X_test)\n\n    #Calcular la precisión\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"{name} - Accuracy: {accuracy}\")\n\n\n","metadata":{"id":"2buA8RNySAys","executionInfo":{"status":"ok","timestamp":1709201744447,"user_tz":-60,"elapsed":129426,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"35c3ce7e-bd3d-4d1b-b6a2-cca044849f76"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los mejores resultados los obtenemos con el modelo de SVC, Regresión Logística y Random Forest.","metadata":{"id":"ynOsRNxR1xYL"}},{"cell_type":"markdown","source":"Probamos ahora con los puntajes obtenidos con TextBlob:","metadata":{"id":"60m8GAo1X4mV"}},{"cell_type":"code","source":"#Utilizar los puntajes de sentimiento obtenidos por TextBlob\ny_textblob = df['Sentiment_TextBlob']\ny_textblob = y_textblob[:X.shape[0]]  # Asegurar que X y y_textblob tengan el mismo número de muestras\n\n#Dividir los datos en conjunto de entrenamiento y conjunto de prueba\nX_train_tb, X_test_tb, y_train_tb, y_test_tb = train_test_split(X, y_textblob, test_size=0.2, random_state=42)\n\n#Inicializar modelos de clasificación\nmodels = [\n    LogisticRegression(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    SVC()\n]\n\n#Nombres de los modelos para identificarlos en la salida\nmodel_names = [\n    \"Regresión Logística\",\n    \"K Vecinos Más Cercanos\",\n    \"Árbol de Decisión\",\n    \"Bosque Aleatorio\",\n    \"SVM\"\n]\n\n#Iterar sobre los modelos y entrenarlos con los puntajes de TextBlob\nfor model, name in zip(models, model_names):\n    #Entrenar el modelo\n    model.fit(X_train_tb, y_train_tb)\n\n    #Predecir sobre el conjunto de prueba\n    y_pred_tb = model.predict(X_test_tb)\n\n    #Calcular la precisión\n    accuracy_tb = accuracy_score(y_test_tb, y_pred_tb)\n    print(f\"{name} - Accuracy (TextBlob): {accuracy_tb}\")\n","metadata":{"id":"Uli8a7raX8IS","executionInfo":{"status":"ok","timestamp":1709202057182,"user_tz":-60,"elapsed":115713,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"c0479a46-8d16-4cb5-a42a-9a0e35f22f0d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que los resultados obtenidos con TextBlob son ligeramente superiores a los obtenidos con Vader. Los modelos que mejor funcionan son los de Regresión Logística y SVM, por encima del 0.55 en ambos casos.","metadata":{"id":"e5y125qpYi5_"}},{"cell_type":"markdown","source":"Representación gráfica de los resultados obtenidos por Vader y Textblob:","metadata":{"id":"kwOUe6_UZHM4"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Paleta de colores pastel\ncolors_pastel = sns.color_palette(\"pastel\")\n\n#Precisión de los modelos con Vader\naccuracies_vader = [0.43024218588283997, 0.4055061063961913, 0.40478161871248186, 0.4254812668184641, 0.4339681225419168]\n\n#Precisión de los modelos con TextBlob\naccuracies_textblob = [0.5555785551645622, 0.4830262885530946, 0.440695508176361, 0.49751604222728213, 0.5555785551645622]\n\n#Nombres de los modelos\nmodel_names = [\n    \"Regresión Logística\",\n    \"K Vecinos Más Cercanos\",\n    \"Árbol de Decisión\",\n    \"Bosque Aleatorio\",\n    \"SVM\"\n]\n\n#Crear el gráfico de barras horizontal con la paleta de colores pastel\nplt.figure(figsize=(10, 6))\nplt.barh(model_names, accuracies_vader, color=colors_pastel, label='Vader')\nplt.barh(model_names, accuracies_textblob, color=colors_pastel, alpha=0.5, label='TextBlob')\nplt.xlabel('Accuracy')\nplt.title('Comparación de Precisión de Modelos con Vader y TextBlob')\nplt.legend()\nplt.gca().invert_yaxis()  # Invertir el eje y para que el modelo superior esté en la parte superior\nplt.show()\n\n","metadata":{"id":"Q4tP96SybbBS","executionInfo":{"status":"ok","timestamp":1709202933821,"user_tz":-60,"elapsed":1662,"user":{"displayName":"Mar Burgueño","userId":"13978725650257904206"}},"outputId":"27379c28-8e2f-4f33-b1ad-c77bf07a5398"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los modelos de Regresión Logística y SVM muestran una precisión considerablemente mayor en comparación con otros modelos, tanto para los puntajes de sentimiento obtenidos por Vader como por TextBlob. Son los modelos más efectivos para clasificar los sentimientos expresados en los diálogos de Friends.\nLos modelos entrenados con los puntajes de TextBlob mostraron una precisión generalmente más alta. Sin embargo, incluso los mejores modelos tienen una precisión que va desde el 40 al 55%, lo que indica que aún hay margen de mejora en la capacidad de estos modelos para capturar la complejidad de los sentimientos expresados en los diálogos de la serie.","metadata":{"id":"1_RCiv1Sb8wz"}},{"cell_type":"markdown","source":"Optimización de parámetros y validación cruzada:","metadata":{"id":"nIhvzhPMcb4o"}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n#Definir los hiperparámetros a ajustar\nparam_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n\n#Inicializar el modelo SVM\nsvm = SVC()\n\n#Realizar la búsqueda de hiperparámetros con validación cruzada\nsvm_grid = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\nsvm_grid.fit(X_train, y_train)\n\n#Obtener los mejores hiperparámetros y la precisión del modelo\nbest_params_svm = svm_grid.best_params_\nbest_score_svm = svm_grid.best_score_\n\nprint(\"Mejores hiperparámetros para SVM:\", best_params_svm)\nprint(\"Precisión del modelo SVM:\", best_score_svm)\n\n","metadata":{"id":"CbW3vUWudffQ"},"execution_count":null,"outputs":[]}]}